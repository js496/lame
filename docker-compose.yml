vllm:
  image: vllm/vllm-openai:latest
  runtime: nvidia
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
  ports:
    - "8000:8000"
  volumes:
    - /path/to/models:/app/model
  environment:
    - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True  # Reduce memory fragmentation
  command: [
    "--model", "neuralmagic/DeepSeek-R1-Distill-Llama-70B-FP8-dynamic",
    "--gpu-memory-utilization", "0.8",  # Reduce GPU memory usage
    "--max-num-seqs", "32",  # Limit concurrent sequences
    "--max-num-batched-tokens", "1024",  # Limit batch size
    "--quantization", "bitsandbytes",  # Use BitsAndBytes quantization
    "--load-format", "bitsandbytes",  # Explicitly set load format
    "--cpu-offload-gb", "4"  # Offload 4 GB to CPU
  ]
  restart: unless-stopped
